---
title: "머신러닝 - 앙상블 학습"
categories: [머신러닝, 학습 전략]
math: true
---

# 🤝 앙상블 학습 (Ensemble Learning)

---

## 🚀 경험: 예측 투표 게임

> 키 180cm인 사람이 농구를 잘할까?

- A: 키가 크면 잘할 것 같아  
- B: 운동부 출신인지가 중요해  
- C: 점프력과 손 크기도 봐야지

세 의견을 모으면 **혼자 판단하는 것보다 정확한 예측** 가능!  
→ 이것이 **앙상블 학습**의 핵심 아이디어.

---

## 🔍 개념: 앙상블이란?

> 여러 모델을 결합해 **더 정확하고 안정적인 예측**을 만드는 방법

각 모델은 다른 시각을 가짐.  
예측 결과를 **평균하거나 투표**해서 최종 결과를 만든다.

---

## ⚙️ 주요 방식

### 🎯 Bagging  
> **여러 모델을 동시에 학습시켜서 평균 or 투표로 예측**

- 데이터 샘플을 무작위로 여러 번 뽑음 (중복 허용)  
- 각각의 데이터로 **독립적인 모델**들을 학습  
- 예측은 **평균(회귀)** 또는 **다수결(분류)**

**대표 모델**: Random Forest  
**효과**: 예측 불안정성(분산)을 줄임

---

### 🔗 Boosting  
> **모델을 순차적으로 학습시켜 오차를 줄여가는 방식**

- 첫 모델이 틀린 부분에 다음 모델이 집중  
- 약한 모델을 이어 붙여 강한 모델을 만듦  
- 예측은 여러 모델의 **가중 평균**

**대표 모델**: Gradient Boosting, XGBoost, LightGBM  
**효과**: 정확도 향상, 하지만 과적합 주의

---

### 🧩 Stacking  
> **서로 다른 모델들의 예측 결과를 다시 학습**

- 다양한 알고리즘 조합 가능  
- 예측 결과를 하나의 메타 모델이 통합 판단

**예**: SVM + Tree + NeuralNet → Logistic Regression

---

## 🧠 암기 요약

| 방식 | 전략 | 대표 모델 |
|------|--------|--------------|
| Bagging | 병렬 학습, 평균/투표 | Random Forest |
| Boosting | 순차 학습, 오차 보완 | XGBoost, LightGBM |
| Stacking | 예측 조합, 메타 학습 | 다양한 조합 가능 |

> 앙상블은 **모델들의 팀플레이**.  
> 약한 모델도 잘 뭉치면 강한 모델보다 낫다.

---

## 📦 언제 쓰일까?

- **분류**: 스팸 탐지, 감정 분석  
- **회귀**: 주택 가격, 기온, 주식 예측  
- Kaggle 대회에서는 사실상 **필수 전략**
