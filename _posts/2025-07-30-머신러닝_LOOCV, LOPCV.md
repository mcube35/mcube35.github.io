---
title: "머신러닝 - LOOCV, LOPCV"
categories: [머신러닝, 교차검증]
math: true
---

# 📚 두 교차검증 방식 LOOCV & LOPCV

---

## 🧬 LOOCV (Leave-One-Out Cross Validation)

말 그대로 **하나 빼고 다 학습**하는 방식이야.

* 데이터셋이 N개라면, N개의 fold를 만드는 것과 같아.
* 매번 하나의 샘플만 검증용으로 빼고, 나머지 N-1개로 학습.
* 이 과정을 N번 반복하고, N개의 결과를 평균 냄.

### 예:

```text
전체 데이터: [A, B, C, D]

1번: 학습 [B, C, D], 검증 A  
2번: 학습 [A, C, D], 검증 B  
3번: 학습 [A, B, D], 검증 C  
4번: 학습 [A, B, C], 검증 D
```

### 장점

* 데이터를 거의 모두 학습에 사용 → **편향(bias)↓**
* 검증 샘플이 매번 하나뿐 → **분산(variance)↑**

### 단점

* **계산량 폭발**: 데이터 많으면 학습 횟수도 많음
* 예측 결과가 **불안정**할 수 있음 (특히 노이즈 많은 데이터에서)

---

## 🔍 LOPCV (Leave-P-Out Cross Validation)

LOOCV의 일반화 버전이야.
이번엔 "하나"가 아니라 **P개를 빼고** 나머지로 학습하는 거지.

* 데이터셋에서 가능한 **모든 P개 조합**을 검증 세트로 사용.
* 남은 N–P개로 학습, P개로 검증.
* 이걸 가능한 모든 조합에 대해 반복.

예를 들어:

* 데이터가 5개, P=2라면
  검증 조합은 (A,B), (A,C), (A,D), ... (D,E) 등 총 10가지 → **조합 수는 `C(N, P)`**

### 장점

* 다양한 검증 패턴으로 **모델의 강건성 테스트** 가능

### 단점

* P가 조금만 커져도 조합 수가 기하급수적으로 증가
  → **실제로는 거의 안 씀**

---

## ⚖️ 요약 비교

| 방법     | 검증 샘플 수 | 반복 횟수   | 계산 비용 | 장점               | 단점                    |
| ------ | ------- | ------- | ----- | ---------------- | --------------------- |
| K-Fold | N/K     | K       | 보통    | 효율적, 유연한 구성 가능   | K 설정에 따라 편향/분산 트레이드오프 |
| LOOCV  | 1       | N       | 높음    | 편향 적고, 모든 데이터 활용 | 계산량 큼, 분산 큼           |
| LOPCV  | P       | C(N, P) | 매우 높음 | 다양한 평가 가능        | 현실적으로 느리고 무거움         |

---

LOOCV는 **데이터가 적을 때**만 쓰는 게 현실적이고,
LOPCV는 보통 **이론 연구용**으로만 남아 있는 경우가 많아.