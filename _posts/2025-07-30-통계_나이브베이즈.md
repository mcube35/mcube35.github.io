---
title: "통계 - 나이브 베이즈"
categories: [통계]
math: true
---

# 📚 나이브 베이즈(Naive Bayes)

---

# 문제: 스팸메일 분류기 만들기

이메일 필터를 만들고 있다. 아래는 나의 훈련 데이터다:

| 이메일 | 단어들                            | 스팸 여부 |
| --- | ------------------------------ | ----- |
| 1   | "cheap", "viagra", "offer"     | 스팸    |
| 2   | "cheap", "replica", "watch"    | 스팸    |
| 3   | "meeting", "schedule", "today" | 일반    |
| 4   | "project", "deadline", "today" | 일반    |

이제 새로운 이메일이 왔다:  
**"cheap", "today"**  
이 이메일이 스팸일 확률을 **나이브 베이즈(Naive Bayes)** 로 계산해보자.

---

## 나이브 베이즈:

1. 각 클래스의 사전확률 $P(\text{스팸}),\ P(\text{일반})$ 을 계산  
2. 각 단어의 조건부 확률 $P(\text{cheap} \mid \text{스팸}),\ P(\text{today} \mid \text{스팸})$ 을 계산해서  
3. 정규화 없이 비교해서 **스팸일 확률**이 더 높은지 **일반일 확률**이 더 높은지 판단  
4. 최종적으로 `"cheap", "today"` 라는 단어가 들어간 이메일이 스팸일 확률을 계산

---

### 1단계: 사전확률 계산 (Prior Probability)

우리는 총 4개의 메일을 읽고 있고, 그중 2개는 스팸이야.

$$
P(\text{스팸}) = \frac{2}{4} = 0.5,\quad P(\text{일반}) = \frac{2}{4} = 0.5
$$

---

### 2단계: 조건부 확률 계산 (Likelihood)

**조건부 확률 계산하기 전에 라플라스 스무딩이란걸 알아야한다**

#### 라플라스 스무딩 (Laplace Smoothing)

**왜 쓰는가?**  
나이브 베이즈 분류기를 쓰다가 어떤 단어가 특정 클래스에서는 한 번도 나타나지 않은 경우  
그 단어의 조건부 확률이 0이 되면서 전체 확률이 몽땅 0이 되어버린다.

예:  
cheap이 한 번도 안 나왔을 경우:  
$P(\text{문서}|\text{스팸}) \propto P(\text{cheap}|\text{스팸}) \times P(\text{today}|\text{스팸}) = 0$

한 단어 때문에 전체 판단이 무력화되는 것!  
→ **이걸 해결하기 위한 게 라플라스 스무딩!**

**어떻게 쓰는가?**  
"해당 클래스에서 단어가 안 나왔더라도 조금은 나왔다고 가정해서" 확률이 0이 되는 사태를 방지한다.

수학적으로는 이렇게:

$$
P(\text{단어} \mid \text{클래스}) = \frac{\text{해당 클래스에서 단어 빈도} + 1}{\text{클래스에서 등장한 전체 단어 수} + V}
$$

여기서

- $+1$은 스무딩 상수 (보통 1, 그래서 **Add-1 smoothing**이라고도 함)
- $V$는 단어 종류의 수 (vocabulary 크기)

---

#### 사전 정보 정리

- 단어의 전체 종류 수 $V = 10$  
- 스팸 메일의 총 단어 수 = 6 (3단어 × 2개 이메일)  
- 일반 메일의 총 단어 수 = 6  

---

#### 스팸 클래스 조건부 확률

| 단어    | 스팸에서 등장한 횟수 | 스무딩 적용 | 확률 |
|--------|----------------|-------------|-----------------------------|
| cheap  | 2              | 2 + 1       | $\frac{3}{6 + 10} = \frac{3}{16}$ |
| today  | 0              | 0 + 1       | $\frac{1}{6 + 10} = \frac{1}{16}$ |

곱해주면:

$$
P(\text{cheap, today} \mid \text{스팸}) = \frac{3}{16} \times \frac{1}{16} = \frac{3}{256}
$$

---

#### 일반 클래스 조건부 확률

| 단어    | 일반에서 등장한 횟수 | 스무딩 적용 | 확률 |
|--------|----------------|-------------|-----------------------------|
| cheap  | 0              | 0 + 1       | $\frac{1}{6 + 10} = \frac{1}{16}$ |
| today  | 2              | 2 + 1       | $\frac{3}{6 + 10} = \frac{3}{16}$ |

곱하면:

$$
P(\text{cheap, today} \mid \text{일반}) = \frac{1}{16} \times \frac{3}{16} = \frac{3}{256}
$$

정리하면, 두 클래스 모두에 대해 조건부 확률 값은 $\frac{3}{256}$로 동일하다.

---

### 3단계: 후험 확률 비교 (Posterior = Likelihood × Prior / Evidence)

위에서 구한 조건부확률과 사전확률을 **베이즈 정리에 따라 곱**해준다.

--- 
$$
\begin{align*}
P(\text{스팸} \mid \text{cheap, today}) &\propto P(\text{cheap, today} \mid \text{스팸}) \cdot P(\text{스팸}) = 0.5 \times \frac{3}{256} = \frac{3}{512} \\
P(\text{일반} \mid \text{cheap, today}) &\propto P(\text{cheap, today} \mid \text{일반}) \cdot P(\text{일반}) = 0.5 \times \frac{3}{256} = \frac{3}{512}
\end{align*}
$$

여기서 기호 $\propto$는 **“정규화 전 비례함”**을 뜻하며,  
실제 확률값을 얻기 위해선 다음 단계에서 **정규화(normalization)** 를 해주면 된다.


---

### 4단계: 확률 구하기 (정규화)

이제 위에서 구한 비례값을 바탕으로 **정규화를 통해 실제 확률값**을 계산한다.  
전체 확률 $P(\text{cheap, today})$ 는 다음과 같다:

$$
P(\text{cheap, today}) = \frac{3}{512} + \frac{3}{512} = \frac{6}{512}
$$

정규화된 후험 확률 (posterior probability)은 다음과 같다:

$$
P(\text{스팸} \mid \text{cheap, today}) = \frac{\frac{3}{512}}{\frac{6}{512}} = \frac{3}{6} = 0.5
$$

$$
P(\text{일반} \mid \text{cheap, today}) = \frac{\frac{3}{512}}{\frac{6}{512}} = \frac{3}{6} = 0.5
$$

즉, 관측된 단어들로 판단했을 때, 이 이메일이 스팸일 확률은 **정확히 0.5**, 일반일 확률도 **0.5**이다.

여기엔 베이즈 정리가 녹아들어있다:
$$
P(\text{스팸} \mid \text{cheap, today}) = \frac{P(\text{cheap, today} \mid \text{스팸}) \cdot P(\text{스팸})}{P(\text{cheap, today})}
$$

---

### 결론

**"cheap", "today"라는 단어가 들어간 이메일은**

> **스팸일 확률이 정확히 0.5**, 일반일 확률도 0.5

## 마치며
지금까지 본 것은 나이브 베이즈의 가장 기본적인 형태고  
이 외에도 다양한 확장 모델들이 존재한다:

- **Multinomial Naive Bayes**:  
  단어의 **등장 횟수(빈도)** 를 반영하는 모델로, 뉴스 분류나 문서 분석에서 자주 쓰임.

- **Bernoulli Naive Bayes**:  
  단어의 **존재 유무만**을 고려하는 방식으로, 이메일 스팸 필터에 특히 잘 어울림.

- **Semi-naive Bayes**:  
  완전한 독립성 가정 대신, **일부 조건부 관계**를 반영하는 변형. 현실 데이터를 더 잘 반영할 수 있다.

- **N-gram 모델**:  
  `"cheap today"`처럼 **단어의 순서나 묶음**을 고려해서 문맥적인 정보까지 담아내는 방식.