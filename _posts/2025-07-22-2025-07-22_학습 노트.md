---
title: "2025-07-22 학습노트"
categories: [학습노트]
date: 2025-07-22
---

# 📚 데이터 분석 & 머신러닝

---

## 1. 데이터 종류

| Category(구분)         | Easily(쉽게 말해) | Example(예시)   |
| -------------------- | ----------------------- | ------------- |
| Structured(정형)       | 표처럼 칸이 딱딱 맞는 것          | 엑셀, DB 테이블    |
| Semi-structured(반정형) | 구조는 있지만 표는 아님           | JSON, XML, 로그 |
| Unstructured(비정형)    | 아무 구조 없는 것              | 이미지, 텍스트, 음성  |

---

## 2. 수치형 vs 범주형

| Type(구분)          | Easily(쉽게 말해) | Example(예시) |
| ----------------- | ----------------------- | ----------- |
| Numeric(수치형)      | 숫자로 된 데이터               | 키, 몸무게, 나이  |
| └ Discrete(이산형)   | 셀 수 있는 숫자               | 사람 수, 주사위   |
| └ Continuous(연속형) | 계속 이어지는 숫자              | 키, 온도       |
| Categorical(범주형)  | 이름이나 그룹                 | 성별, 혈액형     |
| └ Ordinal(순서형)    | 순서가 있는 그룹               | 학점, 만족도     |
| └ Nominal(명목형)    | 순서 없는 그룹                | 혈액형, 지역     |

---

## 3. 통계 기본

- **공분산**: 두 값이 같이 움직이는 정도 (키 크면 몸무게도 크다)
- **상관계수**: 두 값이 얼마나 비슷하게 움직이는지 -1~+1로 표시
- **고유벡터/고유값**: 데이터가 가장 퍼진 방향과 그 크기

---

## 4. 구글 코랩에서 CSV 불러오기

```python
from google.colab import drive
drive.mount('/content/drive')
import pandas as pd
df = pd.read_csv('/content/drive/MyDrive/파일경로/파일명.csv')
```

---

## 5. 피처 엔지니어링

**머신러닝이나 데이터 분석에서 모델의 성능을 향상시키기 위해 데이터를 가공하는 과정**

### 용어 정리

- **피처**: 예측에 쓰는 정보 (키, 몸무게)
- **타겟**: 예측하고 싶은 값 (집값, 점수)
- **레이블**: 예측하고 싶은 그룹 (스팸/정상)
- **클래스**: 레이블의 종류 (고양이, 강아지)
- **다중공선성**: 여러 피처가 서로 너무 비슷해서(강하게 상관되어서) 모델이 불안정해지는 현상
- **잠재공간**: 데이터의 중요한 패턴만 뽑아낸 숨겨진 공간 (머신러닝 돌리다가 자연스럽게 생기는 공간)
- **구간 분할**: 숫자를 등급이나 그룹으로 나누기 (예: 점수로 A/B/C/D/F)

### 피처 엔지니어링 기법
* **Feature Scaling (스케일링)**
* **Encoding (인코딩)**
* **Missing Value Handling (결측치 처리)**
* **Feature Creation (피처 생성)**
* **Feature Extraction (피처 추출)**
* **Feature Selection (피처 선택)**

---

### 5-1. Feature Scaling (스케일링)

**변수 단위를 맞추는 작업**

| Method (방법)                  | Easily (쉽게 말해)       | When to Use (언제 사용?) |
| ---------------------------- | ------------------------------ | -------------------- |
| **Standardization (표준화)**        | 평균을 0, 표준편차를 1로 변환             | 데이터가 정규분포일 때         |
| **Min-Max Scaling (최소-최대 스케일링)** | 최솟값을 0, 최댓값을 1로 변환해 [0,1]로 조정 | 값의 범위를 맞추고 싶을 때      |
| **Robust Scaling (로버스트 스케일링)**   | 중앙값 기준으로 변환, 이상치에 강함           | 이상치가 많을 때            |
| **Log Transformation (로그 변환)**   | 큰 값의 영향을 줄여줌                   | 오른쪽으로 치우친 분포         |

---

### 5-2. Encoding (인코딩)

**범주형 데이터를 수치로 바꾸는 것**

| Method (방법)         | When to Use (언제 사용?)           | 	Example (예시)                |
|--------------|---------------------|---------------------|
| **Label Encoding (레이블 인코딩)**| 순서 있는 그룹      | 학점: A→0, B→1, ...|
| **One-Hot Encoding (원-핫 인코딩)** | 순서 없는 그룹      | 색깔: 빨강→[1,0,0] 파랑→[0,1,0] |
| **Target Encoding (타겟 인코딩)**  | 그룹이 너무 많을 때 | 지역별 평균 집값    |

> **주의**: 원-핫은 그룹 많으면 너무 복잡, 타겟 인코딩은 과적합 위험!

---

### 5-3. Missing Value Handling (결측치 처리)

**비어있는 값을 적절히 채우거나 제거**

#### 결측치 패턴

| Pattern (패턴) | Meaning (의미)    | Difficulty (처리 난이도) |
| ------------ | --------------- | ------------------- |
| **MCAR**     | 완전 무작위 결측       | 쉬움 👍               |
| **MAR**      | 다른 변수에 따라 결측 발생 | 중간 🟡               |
| **MNAR**     | 자기 변수 자체가 결측 원인 | 어려움 ⚠️              |

#### 결측치 처리 전략

| Strategy (전략)                       | Feature (특징)                    | Situation (적합 상황) |
| ----------------------------------- | ------------------------------- | ----------------- |
| **Drop (삭제)**                       | 결측값 있는 행/열 제거                   | 결측 적을 때           |
| **Mean/Median Imputation (평균/중앙값)** | 수치형 채우기 (단순 대체)                 | 빠르게 처리할 때         |
| **Mode Imputation (최빈값)**           | 범주형 채우기                         | 카테고리일 때           |
| **Interpolation (보간)**              | 앞뒤 값으로 채우기 (시계열)                | 시간 데이터에 좋음        |
| **Predictive Imputation (예측 대체)**   | 모델로 결측 예측하여 채움                  | 정확도 중요할 때         |
| **Special Value/Masking (특수값/마스킹)** | `unknown`, `is_null` 등으로 명시적 처리 | 결측 자체도 정보일 때      |

---

### 5-4. Feature Creation (피처 생성)

**도메인 지식을 활용해서 직접 새로운 변수를 만듦**

| Type (종류)                        | Idea (핵심 아이디어)       | Example (간단 예시) |
| -------------------------------- | -------------------- | --------------- |
| **Numeric Combination (수치형 조합)**     | 수치형 변수들로 새 변수 만들기    | `가격 * 수량 = 총액`  |
| **Categorical Combination (범주형 조합)** | 범주형 변수 결합해서 새 그룹 만들기 | `성별+지역`         |
| **Time Derivation (시간 파생)**          | 날짜/시간에서 의미 있는 값 추출   | `요일`, `주말 여부`   |
| **Binary Variable (이진 변수)**          | 기준으로 0 또는 1 나누기      | `연봉 > 5000 → 1` |
| **Group Statistics (그룹 통계)**         | 그룹별 평균, 최대값 등 통계 활용  | `지역별 평균 소득`     |

---

### 5-5. Feature Extraction (피처 추출)

**기존 피처들을 조합해서 새로운 변수로 바꾸는 것**

| Method (방법)                             | Easily (쉽게 말해) | Example (예시)       |
| --------------------------------------- | -------------------------- | ------------------ |
| **PCA: Principal Component Analysis (주성분 분석)**   | 정보의 손실을 최소화하며 차원 축소하기      | 100개의 점수를 2개로 요약하기 |
| **LDA: Linear Discriminant Analysis (선형 판별 분석)** | 그룹 간 구분이 잘 되도록 차원 축소하기     | 고양이와 강아지를 잘 구분하기   |


---

### 5-6. Feature Selection (피처 선택)

**많은 변수를 다 쓰면 노이즈가 많아지니까 중요한 변수만 뽑는 작업**

#### 필터 기법
- **상관계수**: 타겟과 비슷하게 움직이는 피처만 선택
- **카이제곱**: 범주형끼리 연관성 있는 것만 선택
- **분산 임계값**: 값이 거의 안 바뀌는 피처 제거

#### 래퍼 기법
- **순방향 선택**: 하나씩 추가해서 성능 좋아지면 선택
- **역방향 제거**: 다 넣고 하나씩 빼서 성능 나빠지면 다시 넣음
- **재귀적 제거(RFE)**: 중요도 낮은 것부터 반복적으로 제거

#### 임베디드 기법

| Method(방법)            | Easily (쉽게 말해)     | Pros/Cons(장점/단점) | Example(예시) |
| --------------------- | --------------------------- | ---------------- | ----------- |
| Lasso(라쏘)             | L1 정규화로 불필요한 피처 자동으로 0으로 만듦 | 해석 쉬움, 일부만 선택    | 집값 예측       |
| Ridge(릿지)             | L2 정규화로 모든 피처 계수 작게 만듦      | 과적합 방지, 선택 없음    | 다중공선성 해결    |
| Elastic Net(일래스틱넷)    | L1 + L2 섞음                  | 조정 복잡            | 유전자 데이터     |
| Random Forest(랜덤포레스트) | 여러 트리로 중요도 평균냄              | 비선형도 잘 잡음        | 복잡한 분류/회귀   |

##### 정규화 수식
- **L1**: 계수 절댓값 합 제한
- **L2**: 계수 제곱합 제한